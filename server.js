require('dotenv').config();
require('dotenv').config({ path: '.env.local' });

const express = require('express');
const WebSocket = require('ws');
const http = require('http');
const cors = require('cors');
const path = require('path');
const OpenAI = require('openai');
const { PollyClient, SynthesizeSpeechCommand } = require('@aws-sdk/client-polly');
const { 
    TranscribeStreamingClient, 
    StartStreamTranscriptionCommand 
} = require('@aws-sdk/client-transcribe-streaming');

const app = express();
const server = http.createServer(app);
const wss = new WebSocket.Server({ server });

// OpenAI ì„¤ì • (ëŒ€í™” ìƒì„±ìš©)
const openai = new OpenAI({
    apiKey: process.env.OPENAI_API_KEY,
});

// AWS ì„¤ì •
const awsConfig = {
    region: process.env.AWS_REGION || 'ap-northeast-2',
    credentials: {
        accessKeyId: process.env.AWS_ACCESS_KEY_ID,
        secretAccessKey: process.env.AWS_SECRET_ACCESS_KEY,
    },
};

const polly = new PollyClient(awsConfig);
const transcribeClient = new TranscribeStreamingClient(awsConfig);

// ë¯¸ë“¤ì›¨ì–´
app.use(cors());
app.use(express.json());
app.use(express.static('public'));

// í™œì„± ì—°ê²° ê´€ë¦¬
const activeConnections = new Map();

// ë¼ìš°íŠ¸
app.get('/', (req, res) => {
    res.sendFile(path.join(__dirname, 'public', 'index.html'));
});

app.get('/health', (req, res) => {
    res.json({ status: 'ok' });
});

// WebSocket ì—°ê²° ì²˜ë¦¬
wss.on('connection', (ws) => {
    const sessionId = Date.now().toString() + Math.random().toString(36).substr(2, 9);
    console.log(`New connection: ${sessionId}`);
    
    const connection = {
        ws: ws,
        transcribeStream: null,
        conversationHistory: [],
        isProcessing: false,
        audioChunks: []
    };
    
    activeConnections.set(sessionId, connection);

    ws.on('message', async (message) => {
        try {
            const data = JSON.parse(message.toString());
            
            switch (data.type) {
                case 'start-call':
                    await startTranscribeStream(sessionId);
                    ws.send(JSON.stringify({
                        type: 'call-ready',
                        sessionId: sessionId
                    }));
                    break;
                    
                case 'audio-stream':
                    await handleAudioStream(sessionId, data.audioData);
                    break;
                    
                case 'stop-tts':
                    // TTS ì¤‘ë‹¨ ì‹ í˜¸ ì²˜ë¦¬
                    console.log('TTS stop signal received');
                    break;
                    
                case 'end-call':
                    await endTranscribeStream(sessionId);
                    // ì§„í–‰ ì¤‘ì¸ ì²˜ë¦¬ ì¤‘ë‹¨
                    const connection = activeConnections.get(sessionId);
                    if (connection) {
                        connection.isProcessing = false;
                        connection.audioChunks = []; // ëŒ€ê¸° ì¤‘ì¸ ì˜¤ë””ì˜¤ ì²­í¬ ì‚­ì œ
                        console.log('Call ended - stopped processing for session:', sessionId);
                    }
                    break;
            }
            
        } catch (error) {
            console.error('WebSocket message error:', error);
        }
    });

    ws.on('close', () => {
        console.log(`Connection closed: ${sessionId}`);
        const connection = activeConnections.get(sessionId);
        if (connection) {
            connection.isProcessing = false; // ëª¨ë“  ì²˜ë¦¬ ì¤‘ë‹¨
            connection.audioChunks = []; // ëŒ€ê¸° ì¤‘ì¸ ì˜¤ë””ì˜¤ ì‚­ì œ
        }
        endTranscribeStream(sessionId);
        activeConnections.delete(sessionId);
    });
});

// AWS Transcribe ìŠ¤íŠ¸ë¦¼ ì‹œì‘
async function startTranscribeStream(sessionId) {
    const connection = activeConnections.get(sessionId);
    if (!connection) return;

    try {
        console.log(`Starting Transcribe stream for session: ${sessionId}`);
        
        // ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ìƒì„±
        const audioStream = async function* () {
            while (connection.audioChunks.length > 0) {
                const chunk = connection.audioChunks.shift();
                yield { AudioEvent: { AudioChunk: chunk } };
            }
        };

        const command = new StartStreamTranscriptionCommand({
            LanguageCode: 'ko-KR',
            MediaEncoding: 'pcm',
            MediaSampleRateHertz: 16000,
            AudioStream: audioStream()
        });

        const response = await transcribeClient.send(command);
        
        // ì‹¤ì‹œê°„ ì „ì‚¬ ê²°ê³¼ ì²˜ë¦¬
        if (response.TranscriptResultStream) {
            for await (const event of response.TranscriptResultStream) {
                if (event.TranscriptEvent) {
                    const results = event.TranscriptEvent.Transcript.Results;
                    if (results && results.length > 0) {
                        const result = results[0];
                        if (!result.IsPartial && result.Alternatives && result.Alternatives.length > 0) {
                            const transcript = result.Alternatives[0].Transcript;
                            console.log('Transcription result:', transcript);
                            await handleTranscription(sessionId, transcript);
                        }
                    }
                }
            }
        }
        
    } catch (error) {
        console.error('Transcribe stream error:', error);
    }
}

// ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬
async function handleAudioStream(sessionId, audioData) {
    const connection = activeConnections.get(sessionId);
    if (!connection || !audioData) return;

    // PCM ë°ì´í„°ë¥¼ Bufferë¡œ ë³€í™˜
    const audioBuffer = Buffer.from(new Int16Array(audioData).buffer);
    
    // ì˜¤ë””ì˜¤ ì²­í¬ë¥¼ íì— ì¶”ê°€
    connection.audioChunks.push(audioBuffer);
    
    // ë” ë§ì€ ë°ì´í„°ê°€ ìŒ“ì˜€ì„ ë•Œë§Œ ì²˜ë¦¬ (1ì´ˆ ë¶„ëŸ‰ìœ¼ë¡œ ì¦ê°€)
    if (connection.audioChunks.length >= 16) { // 16kHz * 1ì´ˆ / 1024 samples per chunk
        await processAudioChunks(sessionId);
    }
}

// ì˜¤ë””ì˜¤ ì²­í¬ ì¼ê´„ ì²˜ë¦¬
async function processAudioChunks(sessionId) {
    const connection = activeConnections.get(sessionId);
    if (!connection || connection.isProcessing) return;

    connection.isProcessing = true;
    
    try {
        // ì²­í¬ë“¤ì„ ê²°í•©
        const combinedAudio = Buffer.concat(connection.audioChunks);
        connection.audioChunks = [];
        
        console.log('Processing audio chunks, total size:', combinedAudio.length);
        
        // ì˜¤ë””ì˜¤ ë°ì´í„° ìœ íš¨ì„± ê²€ì¦
        if (!isValidAudioData(combinedAudio)) {
            console.log('Invalid audio data detected, skipping processing');
            return;
        }
        
        // AWS Transcribeë¡œ ì „ì†¡ (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ ì‚¬ìš©)
        await processWithTranscribe(sessionId, combinedAudio);
        
    } catch (error) {
        console.error('Audio processing error:', error);
    } finally {
        connection.isProcessing = false;
    }
}

// AWS Transcribeë¡œ ìŒì„± ì¸ì‹ (ì„ì‹œ êµ¬í˜„ - ì‹¤ì œë¡œëŠ” ìŠ¤íŠ¸ë¦¬ë° ì‚¬ìš©)
async function processWithTranscribe(sessionId, audioBuffer) {
    const connection = activeConnections.get(sessionId);
    if (!connection) return;

    try {
        // ì„ì‹œ: OpenAI Whisper ì‚¬ìš© (AWS Transcribe ìŠ¤íŠ¸ë¦¬ë° ì™„ì „ êµ¬í˜„ê¹Œì§€)
        const fs = require('fs');
        const tempFilePath = path.join(__dirname, `temp_${sessionId}_${Date.now()}.wav`);
        
        // PCM to WAV ë³€í™˜
        const wavBuffer = createWavBuffer(audioBuffer);
        fs.writeFileSync(tempFilePath, wavBuffer);
        
        const transcription = await openai.audio.transcriptions.create({
            file: fs.createReadStream(tempFilePath),
            model: 'whisper-1',
            language: 'ko'
        });
        
        fs.unlinkSync(tempFilePath);
        
        if (transcription.text.trim()) {
            await handleTranscription(sessionId, transcription.text);
        }
        
    } catch (error) {
        console.error('Transcribe processing error:', error);
    }
}

// ì˜¤ë””ì˜¤ ë°ì´í„° ìœ íš¨ì„± ê²€ì¦
function isValidAudioData(audioBuffer) {
    if (!audioBuffer || audioBuffer.length === 0) {
        return false;
    }
    
    // ìµœì†Œ ê¸¸ì´ ì²´í¬ (0.5ì´ˆ ì´ìƒ)
    if (audioBuffer.length < 16000) { // 16kHz * 0.5ì´ˆ * 2 bytes
        console.log('Audio too short:', audioBuffer.length);
        return false;
    }
    
    // RMS ì—ë„ˆì§€ ê³„ì‚°
    let sum = 0;
    const samples = audioBuffer.length / 2; // 16-bit samples
    
    for (let i = 0; i < audioBuffer.length; i += 2) {
        const sample = audioBuffer.readInt16LE(i);
        sum += sample * sample;
    }
    
    const rms = Math.sqrt(sum / samples);
    const normalizedRMS = rms / 32768; // 16-bit ì •ê·œí™”
    
    console.log('Audio RMS:', normalizedRMS);
    
    // ìµœì†Œ ì—ë„ˆì§€ ì„ê³„ê°’ (ë°°ê²½ ì†ŒìŒë³´ë‹¤ ì¶©ë¶„íˆ ë†’ì•„ì•¼ í•¨)
    const minEnergyThreshold = 0.01;
    
    if (normalizedRMS < minEnergyThreshold) {
        console.log('Audio energy too low, likely silence or noise');
        return false;
    }
    
    return true;
}

// PCMì„ WAVë¡œ ë³€í™˜
function createWavBuffer(pcmBuffer) {
    const sampleRate = 16000;
    const numChannels = 1;
    const bitsPerSample = 16;
    
    const dataLength = pcmBuffer.length;
    const headerLength = 44;
    const totalLength = headerLength + dataLength;
    
    const wavBuffer = Buffer.alloc(totalLength);
    
    // WAV í—¤ë”
    wavBuffer.write('RIFF', 0);
    wavBuffer.writeUInt32LE(totalLength - 8, 4);
    wavBuffer.write('WAVE', 8);
    wavBuffer.write('fmt ', 12);
    wavBuffer.writeUInt32LE(16, 16);
    wavBuffer.writeUInt16LE(1, 20);
    wavBuffer.writeUInt16LE(numChannels, 22);
    wavBuffer.writeUInt32LE(sampleRate, 24);
    wavBuffer.writeUInt32LE(sampleRate * numChannels * bitsPerSample / 8, 28);
    wavBuffer.writeUInt16LE(numChannels * bitsPerSample / 8, 32);
    wavBuffer.writeUInt16LE(bitsPerSample, 34);
    wavBuffer.write('data', 36);
    wavBuffer.writeUInt32LE(dataLength, 40);
    
    // PCM ë°ì´í„° ë³µì‚¬
    pcmBuffer.copy(wavBuffer, headerLength);
    
    return wavBuffer;
}

// ì „ì‚¬ ê²°ê³¼ ì²˜ë¦¬
async function handleTranscription(sessionId, transcript) {
    const connection = activeConnections.get(sessionId);
    if (!connection) {
        console.log('Connection not found, skipping transcription handling for session:', sessionId);
        return;
    }

    // í†µí™”ê°€ ì¢…ë£Œëœ ê²½ìš° ì „ì‚¬ ê²°ê³¼ ì²˜ë¦¬í•˜ì§€ ì•ŠìŒ
    if (!connection.isProcessing) {
        console.log('Call ended, skipping transcription handling for session:', sessionId);
        return;
    }

    console.log('Handling transcription:', transcript);
    
    try {
        // AI ì‘ë‹µ ìƒì„±
        const aiResponse = await generateAIResponse(transcript, connection.conversationHistory);
        if (!aiResponse) return;
        
        // ëŒ€í™” íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
        connection.conversationHistory.push(
            { role: 'user', content: transcript },
            { role: 'assistant', content: aiResponse }
        );
        
        // íˆìŠ¤í† ë¦¬ ê¸¸ì´ ì œí•œ (ë©”ëª¨ë¦¬ ê´€ë¦¬)
        if (connection.conversationHistory.length > 20) {
            connection.conversationHistory.splice(0, 2);
        }
        
        // TTS ìƒì„± ë° ì „ì†¡
        await generateAndStreamTTS(sessionId, aiResponse);
        
        // ëŒ€í™” ë¡œê·¸ ì „ì†¡
        connection.ws.send(JSON.stringify({
            type: 'conversation',
            user: transcript,
            assistant: aiResponse,
            timestamp: Date.now()
        }));
        
    } catch (error) {
        console.error('Transcription handling error:', error);
    }
}

// AI ì‘ë‹µ ìƒì„±
async function generateAIResponse(userMessage, history) {
    try {
        const completion = await openai.chat.completions.create({
            model: 'gpt-3.5-turbo',
            messages: [
                { 
                    role: 'system', 
                    content: 'ë‹¹ì‹ ì€ ì‹¤ì‹œê°„ ìŒì„± ëŒ€í™”ë¥¼ í•˜ëŠ” ì¹œê·¼í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ìì—°ìŠ¤ëŸ½ê³  ê°„ê²°í•˜ê²Œ ëŒ€í™”í•˜ì„¸ìš”. ì „í™” í†µí™”í•˜ë“¯ì´ ë°˜ì‘í•˜ê³ , ìƒëŒ€ë°©ì´ ë§ì„ ëŠì„ ìˆ˜ ìˆìŒì„ ê³ ë ¤í•´ í•µì‹¬ë¶€í„° ë§í•˜ì„¸ìš”.' 
                },
                ...history,
                { role: 'user', content: userMessage }
            ],
            max_tokens: 150,
            temperature: 0.8
        });
        
        return completion.choices[0].message.content;
        
    } catch (error) {
        console.error('AI response error:', error);
        return null;
    }
}

// AWS Polly TTS ìƒì„± ë° ìŠ¤íŠ¸ë¦¬ë°
async function generateAndStreamTTS(sessionId, text) {
    const connection = activeConnections.get(sessionId);
    if (!connection) {
        console.log('Connection not found, skipping TTS for session:', sessionId);
        return;
    }
    
    // í†µí™”ê°€ ì¢…ë£Œë˜ì—ˆê±°ë‚˜ ì²˜ë¦¬ ì¤‘ë‹¨ëœ ê²½ìš° TTS ìƒì„±í•˜ì§€ ì•ŠìŒ
    if (!connection.isProcessing) {
        console.log('Call ended or processing stopped, skipping TTS for session:', sessionId);
        return;
    }
    
    try {
        console.log('Generating TTS for:', text.substring(0, 50) + '...');
        
        const command = new SynthesizeSpeechCommand({
            Text: text,
            OutputFormat: 'mp3',
            VoiceId: 'Seoyeon',
            Engine: 'neural',
            SampleRate: '22050'
        });
        
        const ttsResult = await polly.send(command);
        
        if (ttsResult.AudioStream) {
            const chunks = [];
            for await (const chunk of ttsResult.AudioStream) {
                chunks.push(chunk);
            }
            const audioBuffer = Buffer.concat(chunks);
            
            // ì‹¤ì‹œê°„ ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¬ë°
            connection.ws.send(JSON.stringify({
                type: 'audio-response',
                audioData: audioBuffer.toString('base64'),
                contentType: 'audio/mp3'
            }));
            
            console.log('TTS audio sent to client');
        }
        
    } catch (error) {
        console.error('TTS error:', error);
    }
}

// Transcribe ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ
async function endTranscribeStream(sessionId) {
    const connection = activeConnections.get(sessionId);
    if (!connection) return;

    if (connection.transcribeStream) {
        try {
            connection.transcribeStream.destroy();
            connection.transcribeStream = null;
            console.log(`Transcribe stream ended for session: ${sessionId}`);
        } catch (error) {
            console.error('Error ending transcribe stream:', error);
        }
    }
}

const PORT = process.env.PORT || 3000;
server.listen(PORT, () => {
    console.log(`ğŸ¤ VibeMe WebRTC + AWS Voice Chat running on port ${PORT}`);
    console.log(`ğŸ“ Real-time conversation with AWS Transcribe + Polly enabled`);
});