require('dotenv').config();
require('dotenv').config({ path: '.env.local' });

const express = require('express');
const WebSocket = require('ws');
const http = require('http');
const cors = require('cors');
const path = require('path');
const fs = require('fs');
const OpenAI = require('openai');
const { PollyClient, SynthesizeSpeechCommand } = require('@aws-sdk/client-polly');

const app = express();
const server = http.createServer(app);
const wss = new WebSocket.Server({ server });

// OpenAI ì„¤ì •
const openai = new OpenAI({
    apiKey: process.env.OPENAI_API_KEY,
});

// AWS Polly ì„¤ì •
const polly = new PollyClient({
    region: process.env.AWS_REGION || 'ap-northeast-2',
    credentials: {
        accessKeyId: process.env.AWS_ACCESS_KEY_ID,
        secretAccessKey: process.env.AWS_SECRET_ACCESS_KEY,
    },
});

// ë¯¸ë“¤ì›¨ì–´
app.use(cors());
app.use(express.json());
app.use(express.static('public'));

// í™œì„± ì—°ê²° ê´€ë¦¬
const activeConnections = new Map();

// ë¼ìš°íŠ¸
app.get('/', (req, res) => {
    res.sendFile(path.join(__dirname, 'public', 'index.html'));
});

app.get('/health', (req, res) => {
    res.json({ status: 'ok' });
});

// WebRTC ì‹œê·¸ë„ë§ ë° ì‹¤ì‹œê°„ ì˜¤ë””ì˜¤ ì²˜ë¦¬
wss.on('connection', (ws) => {
    const sessionId = Date.now().toString() + Math.random().toString(36).substr(2, 9);
    console.log(`New WebRTC connection: ${sessionId}`);
    
    activeConnections.set(sessionId, {
        ws: ws,
        isProcessing: false,
        conversationHistory: [],
        audioBuffer: [],
        vadTimer: null
    });

    ws.on('message', async (message) => {
        try {
            const data = JSON.parse(message.toString());
            const connection = activeConnections.get(sessionId);
            
            switch (data.type) {
                case 'offer':
                case 'answer':
                case 'ice-candidate':
                    // WebRTC ì‹œê·¸ë„ë§ ì¤‘ê³„
                    ws.send(JSON.stringify(data));
                    break;
                    
                case 'audio-stream':
                    console.log('Received audio-stream message, data length:', data.audioData?.length);
                    await handleAudioStream(sessionId, data.audioData);
                    break;
                    
                case 'start-call':
                    ws.send(JSON.stringify({
                        type: 'call-ready',
                        sessionId: sessionId
                    }));
                    break;
                    
                case 'end-call':
                    if (connection) {
                        connection.isProcessing = false;
                        if (connection.vadTimer) {
                            clearTimeout(connection.vadTimer);
                        }
                    }
                    break;
            }
            
        } catch (error) {
            console.error('WebSocket message error:', error);
        }
    });

    ws.on('close', () => {
        console.log(`WebRTC connection closed: ${sessionId}`);
        const connection = activeConnections.get(sessionId);
        if (connection && connection.vadTimer) {
            clearTimeout(connection.vadTimer);
        }
        activeConnections.delete(sessionId);
    });
});

// ì‹¤ì‹œê°„ ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬
async function handleAudioStream(sessionId, audioData) {
    const connection = activeConnections.get(sessionId);
    if (!connection) {
        console.log('No connection found for session:', sessionId);
        return;
    }

    if (!audioData || audioData.length === 0) {
        console.log('Empty or invalid audio data received');
        return;
    }

    console.log('Received audio stream, length:', audioData.length, 'type:', typeof audioData[0]);

    // ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ ë²„í¼ì— ëˆ„ì 
    connection.audioBuffer.push(...audioData);
    console.log('Total buffer length after push:', connection.audioBuffer.length);
    
    // 3ì´ˆ ë¶„ëŸ‰ì˜ ì˜¤ë””ì˜¤ (16kHz * 3ì´ˆ = 48000 ìƒ˜í”Œ)ê°€ ìŒ“ì´ë©´ ì²˜ë¦¬
    const targetBufferSize = 48000;
    
    if (connection.audioBuffer.length >= targetBufferSize && !connection.isProcessing) {
        console.log('Buffer size reached target, processing audio immediately...');
        await processAccumulatedAudio(sessionId);
        return;
    }
    
    // VAD íƒ€ì´ë¨¸ (ì¹¨ë¬µ ê°ì§€ìš©) - ì²­í¬ ê¸°ë°˜ ì²˜ë¦¬ì™€ ë³„ë„ë¡œ ìš´ì˜
    if (connection.vadTimer) {
        clearTimeout(connection.vadTimer);
    }
    
    // 4ì´ˆ ì¹¨ë¬µ í›„ ê°•ì œ ì²˜ë¦¬ (ë°±ì—…ìš©)
    connection.vadTimer = setTimeout(async () => {
        console.log('VAD timeout triggered, processing remaining audio...');
        await processAccumulatedAudio(sessionId);
    }, 4000);
}

// ëˆ„ì ëœ ì˜¤ë””ì˜¤ ì²˜ë¦¬
async function processAccumulatedAudio(sessionId) {
    const connection = activeConnections.get(sessionId);
    if (!connection || connection.isProcessing || connection.audioBuffer.length === 0) {
        console.log('Skipping audio processing:', {
            hasConnection: !!connection,
            isProcessing: connection?.isProcessing,
            bufferLength: connection?.audioBuffer?.length || 0
        });
        return;
    }
    
    connection.isProcessing = true;
    console.log('Processing accumulated audio, buffer length:', connection.audioBuffer.length);
    
    try {
        // ì˜¤ë””ì˜¤ ë²„í¼ë¥¼ Int16Arrayë¡œ ë³€í™˜ (í´ë¼ì´ì–¸íŠ¸ì—ì„œ ì „ì†¡í•œ í˜•ì‹ê³¼ ì¼ì¹˜)
        const combinedAudio = new Int16Array(connection.audioBuffer);
        connection.audioBuffer = []; // ë²„í¼ ì´ˆê¸°í™”
        
        console.log('Combined audio length:', combinedAudio.length);
        
        if (combinedAudio.length < 8000) { // ìµœì†Œ 0.5ì´ˆ ë¶„ëŸ‰ (16kHz)
            console.log('Audio too short, skipping');
            connection.isProcessing = false;
            return;
        }
        
        // STT ì²˜ë¦¬
        const transcription = await processSTT(combinedAudio);
        if (!transcription) {
            connection.isProcessing = false;
            return;
        }
        
        // AI ì‘ë‹µ ìƒì„±
        const aiResponse = await generateAIResponse(transcription, connection.conversationHistory);
        if (!aiResponse) {
            connection.isProcessing = false;
            return;
        }
        
        // ëŒ€í™” íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
        connection.conversationHistory.push(
            { role: 'user', content: transcription },
            { role: 'assistant', content: aiResponse }
        );
        
        // íˆìŠ¤í† ë¦¬ ê¸¸ì´ ì œí•œ
        if (connection.conversationHistory.length > 20) {
            connection.conversationHistory.splice(0, 2);
        }
        
        // TTS ìƒì„± ë° ìŠ¤íŠ¸ë¦¬ë°
        await generateAndStreamTTS(sessionId, aiResponse);
        
        // ëŒ€í™” ë¡œê·¸ ì „ì†¡
        connection.ws.send(JSON.stringify({
            type: 'conversation',
            user: transcription,
            assistant: aiResponse,
            timestamp: Date.now()
        }));
        
    } catch (error) {
        console.error('Audio processing error:', error);
    } finally {
        connection.isProcessing = false;
    }
}

// ì˜¤ë””ì˜¤ ë²„í¼ ê²°í•©
function combineAudioBuffers(buffers) {
    const totalLength = buffers.reduce((sum, buffer) => sum + buffer.length, 0);
    const combined = new Uint8Array(totalLength);
    let offset = 0;
    
    for (const buffer of buffers) {
        const uint8Buffer = new Uint8Array(buffer);
        combined.set(uint8Buffer, offset);
        offset += uint8Buffer.length;
    }
    
    return combined;
}

// STT ì²˜ë¦¬
async function processSTT(audioBuffer) {
    try {
        console.log('Processing STT for buffer of length:', audioBuffer.length);
        
        // PCM ë°ì´í„°ë¥¼ WAV í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        const wavBuffer = createWavBuffer(audioBuffer);
        
        // ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
        const tempFilePath = path.join(__dirname, `temp_stream_${Date.now()}.wav`);
        fs.writeFileSync(tempFilePath, wavBuffer);
        
        console.log('Saved WAV file:', tempFilePath, 'size:', wavBuffer.length);
        
        const transcription = await openai.audio.transcriptions.create({
            file: fs.createReadStream(tempFilePath),
            model: 'whisper-1',
            language: 'ko'
        });
        
        // ì„ì‹œ íŒŒì¼ ì‚­ì œ
        fs.unlinkSync(tempFilePath);
        
        console.log('STT result:', transcription.text);
        return transcription.text;
        
    } catch (error) {
        console.error('STT error:', error);
        return null;
    }
}

// PCMì„ WAVë¡œ ë³€í™˜
function createWavBuffer(pcmBuffer) {
    const sampleRate = 16000;
    const numChannels = 1;
    const bitsPerSample = 16;
    
    // pcmBufferëŠ” ì´ë¯¸ Int16Arrayì´ë¯€ë¡œ ê¸¸ì´ëŠ” ìƒ˜í”Œ ìˆ˜
    const dataLength = pcmBuffer.length * 2; // 16-bit samples = 2 bytes per sample
    const headerLength = 44;
    const totalLength = headerLength + dataLength;
    
    const wavBuffer = Buffer.alloc(totalLength);
    
    // WAV í—¤ë” ì‘ì„±
    wavBuffer.write('RIFF', 0);
    wavBuffer.writeUInt32LE(totalLength - 8, 4);
    wavBuffer.write('WAVE', 8);
    wavBuffer.write('fmt ', 12);
    wavBuffer.writeUInt32LE(16, 16); // PCM format chunk size
    wavBuffer.writeUInt16LE(1, 20); // PCM format
    wavBuffer.writeUInt16LE(numChannels, 22);
    wavBuffer.writeUInt32LE(sampleRate, 24);
    wavBuffer.writeUInt32LE(sampleRate * numChannels * bitsPerSample / 8, 28);
    wavBuffer.writeUInt16LE(numChannels * bitsPerSample / 8, 32);
    wavBuffer.writeUInt16LE(bitsPerSample, 34);
    wavBuffer.write('data', 36);
    wavBuffer.writeUInt32LE(dataLength, 40);
    
    // PCM ë°ì´í„°ë¥¼ 16-bitë¡œ ë³µì‚¬ (ì´ë¯¸ ì˜¬ë°”ë¥¸ í˜•ì‹)
    for (let i = 0; i < pcmBuffer.length; i++) {
        wavBuffer.writeInt16LE(pcmBuffer[i], headerLength + i * 2);
    }
    
    return wavBuffer;
}

// AI ì‘ë‹µ ìƒì„±
async function generateAIResponse(userMessage, history) {
    try {
        const completion = await openai.chat.completions.create({
            model: 'gpt-3.5-turbo',
            messages: [
                { 
                    role: 'system', 
                    content: 'ë‹¹ì‹ ì€ ì‹¤ì‹œê°„ ìŒì„± ëŒ€í™”ë¥¼ í•˜ëŠ” ì¹œê·¼í•œ AIì…ë‹ˆë‹¤. ìì—°ìŠ¤ëŸ½ê³  ê°„ê²°í•˜ê²Œ ëŒ€í™”í•˜ì„¸ìš”. ì „í™” í†µí™”í•˜ë“¯ì´ ë°˜ì‘í•˜ì„¸ìš”.' 
                },
                ...history,
                { role: 'user', content: userMessage }
            ],
            max_tokens: 100,
            temperature: 0.7
        });
        
        return completion.choices[0].message.content;
        
    } catch (error) {
        console.error('AI response error:', error);
        return null;
    }
}

// TTS ìƒì„± ë° ìŠ¤íŠ¸ë¦¬ë°
async function generateAndStreamTTS(sessionId, text) {
    const connection = activeConnections.get(sessionId);
    if (!connection) return;
    
    try {
        const command = new SynthesizeSpeechCommand({
            Text: text,
            OutputFormat: 'mp3',
            VoiceId: 'Seoyeon',
            Engine: 'neural',
            SampleRate: '22050'
        });
        
        const ttsResult = await polly.send(command);
        
        if (ttsResult.AudioStream) {
            const chunks = [];
            for await (const chunk of ttsResult.AudioStream) {
                chunks.push(chunk);
            }
            const audioBuffer = Buffer.concat(chunks);
            
            // ì‹¤ì‹œê°„ ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¬ë°
            connection.ws.send(JSON.stringify({
                type: 'audio-response',
                audioData: audioBuffer.toString('base64'),
                contentType: 'audio/mp3'
            }));
        }
        
    } catch (error) {
        console.error('TTS error:', error);
    }
}

const PORT = process.env.PORT || 3000;
server.listen(PORT, () => {
    console.log(`ğŸ¤ VibeMe Real-time Voice Chat running on port ${PORT}`);
    console.log(`ğŸ“ WebRTC P2P voice calling enabled`);
});